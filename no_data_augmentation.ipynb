{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import math\n",
    "import numpy\n",
    "import os\n",
    "import pandas\n",
    "import pathlib\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "import sklearn.model_selection\n",
    "import tensorflow\n",
    "import time\n",
    "\n",
    "\n",
    "from files import create_folder, save_fit_history, save_lossgraph, save_figs\n",
    "from metrics import dice_coef, jaccard_distance\n",
    "from model import evaluate, unet_model, get_loss_function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class CreateSequence(tensorflow.keras.utils.Sequence):\n",
    "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.input_img_paths = input_img_paths\n",
    "        self.target_img_paths = target_img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
    "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
    "\n",
    "        x = numpy.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
    "        for j, path in enumerate(batch_input_img_paths):\n",
    "            img = skimage.io.imread(path.resolve())\n",
    "            img = numpy.float32(img/255)\n",
    "            x[j] = img\n",
    "\n",
    "        y = numpy.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"float32\")\n",
    "        for j, path in enumerate(batch_target_img_paths):\n",
    "            mask = skimage.io.imread(path.resolve())\n",
    "            mask = numpy.expand_dims(mask, 2)\n",
    "            mask = numpy.float32(mask/255)\n",
    "            y[j] = mask\n",
    "\n",
    "        return x, y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GPU"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: /physical_device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "gpus = tensorflow.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            print(f\"GPU: {gpu.name}\")\n",
    "            tensorflow.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"channel\": 3,\n",
    "    \"batch_size\": 4,\n",
    "    \"fold\": 5,\n",
    "    \"epochs\": 75,\n",
    "    \"image_size\": 256,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"random_state\": 1234,\n",
    "    \"test_size\": 0.2,\n",
    "    \"val_size\": 0.05,\n",
    "    \"path_dataset\": \"dataset\",\n",
    "    \"path_out\": \"out\",\n",
    "    \"loss_function\": \"dice\",\n",
    "    \"data_augmentation\": False\n",
    "}\n",
    "images_folder = os.path.join(cfg[\"path_dataset\"], \"IMAGEM_ORIGINAL\", \"CONVERTIDAS\", \"RGB\", \"256\", \"OUT\")\n",
    "masks_folder = os.path.join(cfg[\"path_dataset\"], \"MASK\", \"BITMAP\", \"256\", \"OUT\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "x = numpy.array(list([file for file in sorted(pathlib.Path(images_folder).rglob(\"*\")) if file.is_file()]))\n",
    "y = numpy.array(list([file for file in sorted(pathlib.Path(masks_folder).rglob(\"*\")) if file.is_file()]))\n",
    "\n",
    "kf = sklearn.model_selection.KFold(n_splits=cfg[\"fold\"], shuffle=True, random_state=cfg[\"random_state\"])\n",
    "# data = CreateSequence(cfg[\"batch_size\"], (cfg[\"image_size\"], cfg[\"image_size\"]), x, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(285,)\n",
      "(15,)\n",
      "(75,)\n",
      "(375,)\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-11 10:43:33.194367: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-11 10:43:33.194880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-11 10:43:33.195031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-11 10:43:33.195135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-11 10:43:33.497245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-11 10:43:33.497389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-11 10:43:33.497503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-11 10:43:33.497593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9619 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-11 10:43:34.141916: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_2318\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\020FlatMapDataset:1\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
      "2022-09-11 10:43:40.170172: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8500\n",
      "2022-09-11 10:43:40.493854: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/72 [============================>.] - ETA: 0s - loss: 0.4194 - dice_coef: 0.5806 - jaccard_distance: 0.4187 - precision: 0.6445 - recall: 0.9534WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 5400 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-11 10:43:45.232987: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_10706\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021FlatMapDataset:23\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.64121, saving model to out/11-09-2022-10-43-33/0/unet.h5\n",
      "72/72 [==============================] - 13s 71ms/step - loss: 0.4194 - dice_coef: 0.5806 - jaccard_distance: 0.4187 - precision: 0.6445 - recall: 0.9534 - val_loss: 0.6412 - val_dice_coef: 0.3588 - val_jaccard_distance: 0.2202 - val_precision: 0.8445 - val_recall: 0.2094 - lr: 0.0010\n",
      "out/11-09-2022-10-43-33/0/fold0-fit.pckl created\n",
      "out/11-09-2022-10-43-33/0/fold0-lossgraph.png created\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "list_evaluate = list([])\n",
    "current_datetime = datetime.datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "path = os.path.join(cfg[\"path_out\"], current_datetime)\n",
    "create_folder(list([path]))\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(x)):\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    x_train, x_val, y_train, y_val = sklearn.model_selection.train_test_split(x_train, y_train, test_size=cfg[\"val_size\"], random_state=cfg[\"random_state\"])\n",
    "\n",
    "    train_data = CreateSequence(cfg[\"batch_size\"], (cfg[\"image_size\"], cfg[\"image_size\"]), x_train, y_train)\n",
    "    val_data = CreateSequence(cfg[\"batch_size\"], (cfg[\"image_size\"], cfg[\"image_size\"]), x_val, y_val)\n",
    "    test_data = CreateSequence(cfg[\"batch_size\"], (cfg[\"image_size\"], cfg[\"image_size\"]), x_test, y_test)\n",
    "\n",
    "    print(x_train.shape)\n",
    "    print(x_val.shape)\n",
    "    print(x_test.shape)\n",
    "    print(x.shape)\n",
    "\n",
    "    path_fold = os.path.join(path, str(fold))\n",
    "    create_folder(list([path_fold]))\n",
    "\n",
    "    steps_per_epoch = math.ceil(x_train.shape[0] / cfg[\"batch_size\"])\n",
    "    reduce_learning_rate = tensorflow.keras.callbacks.ReduceLROnPlateau(monitor=\"loss\", factor=0.5, patience=3, verbose=1)\n",
    "    filename_model = os.path.join(path_fold, \"unet.h5\")\n",
    "    checkpointer = tensorflow.keras.callbacks.ModelCheckpoint(filename_model, verbose=1, save_best_only=True)\n",
    "    strategy = tensorflow.distribute.MirroredStrategy()\n",
    "\n",
    "    with strategy.scope():\n",
    "        model = unet_model(cfg)\n",
    "        adam_opt = tensorflow.keras.optimizers.Adam(learning_rate=cfg[\"learning_rate\"])\n",
    "        model.compile(optimizer=adam_opt, loss=get_loss_function(cfg[\"loss_function\"]), metrics=[dice_coef, jaccard_distance, tensorflow.keras.metrics.Precision(), tensorflow.keras.metrics.Recall()])\n",
    "\n",
    "    tensorflow.keras.backend.clear_session()\n",
    "    start_time = time.time()\n",
    "    fit = model.fit(train_data, steps_per_epoch=steps_per_epoch, epochs=cfg[\"epochs\"], validation_data=val_data, callbacks=[checkpointer, reduce_learning_rate])\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    save_fit_history(fold, fit, path_fold)\n",
    "    save_lossgraph(fold, fit, path_fold)\n",
    "    # evaluate(train_data)\n",
    "    # list_evaluate.append(evaluate(end_time, fold, model, x_train, x_val, x_test, y_train, y_val, y_test))\n",
    "\n",
    "    # models.append(model)\n",
    "\n",
    "    # model = tensorflow.keras.models.load_model(\"unet_rgb.h5\", custom_objects = {\"dice_loss\": dice_loss, \"dice_coef\": dice_coef, \"jaccard_distance\": jaccard_distance })\n",
    "\n",
    "    # save_figs(cfg, list_images_names, test_index, model, path_fold, x)\n",
    "    break\n",
    "tensorflow.keras.backend.clear_session()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "columns = [\"batch_size\", \"epochs\", \"learning_rate\", \"loss_function\", \"images\", \"masks\", \"len_images\", \"len_masks\", \"channel\", \"image_size\", \"fold\", \"test_size\", \"val_size\", \"random_state\", \"path_dataset\", \"path_out\", \"data_augmentation\"]\n",
    "data = [cfg[\"batch_size\"], cfg[\"epochs\"], cfg[\"learning_rate\"], cfg[\"loss_function\"], images_folder, masks_folder, len(x), len(y), cfg[\"channel\"], cfg[\"image_size\"], cfg[\"fold\"], cfg[\"test_size\"], cfg[\"val_size\"], cfg[\"random_state\"], cfg[\"path_dataset\"], cfg[\"path_out\"], cfg[\"data_augmentation\"]]\n",
    "\n",
    "dataframe_cfg = pandas.DataFrame(data, columns)\n",
    "dataframe_cfg.to_csv(os.path.join(path, \"cfg.csv\"), sep=\";\", na_rep=\" \")\n",
    "dataframe_cfg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_mean(key, list_evaluate):\n",
    "    return str(numpy.mean(list([evaluate[key] for evaluate in list_evaluate])))\n",
    "\n",
    "def get_std(key, list_evaluate):\n",
    "    return str(numpy.std(list([evaluate[key] for evaluate in list_evaluate])))\n",
    "\n",
    "dataframe_mean = pandas.DataFrame({\"mean_train\": [get_mean(\"loss_train\", list_evaluate),\n",
    "                                                  get_mean(\"dice_train\", list_evaluate),\n",
    "                                                  get_mean(\"jaccard_train\", list_evaluate),\n",
    "                                                  get_mean(\"precision_train\", list_evaluate),\n",
    "                                                  get_mean(\"recall_train\", list_evaluate)],\n",
    "                                   \"std_train\": [get_std(\"loss_train\", list_evaluate),\n",
    "                                                  get_std(\"dice_train\", list_evaluate),\n",
    "                                                  get_std(\"jaccard_train\", list_evaluate),\n",
    "                                                  get_std(\"precision_train\", list_evaluate),\n",
    "                                                  get_std(\"recall_train\", list_evaluate)],\n",
    "                                   \"mean_val\": [get_mean(\"loss_val\", list_evaluate),\n",
    "                                                  get_mean(\"dice_val\", list_evaluate),\n",
    "                                                  get_mean(\"jaccard_val\", list_evaluate),\n",
    "                                                  get_mean(\"precision_val\", list_evaluate),\n",
    "                                                  get_mean(\"recall_val\", list_evaluate)],\n",
    "                                   \"std_val\": [get_std(\"loss_val\", list_evaluate),\n",
    "                                                  get_std(\"dice_val\", list_evaluate),\n",
    "                                                  get_std(\"jaccard_val\", list_evaluate),\n",
    "                                                  get_std(\"precision_val\", list_evaluate),\n",
    "                                                  get_std(\"recall_val\", list_evaluate)],\n",
    "                                   \"mean_test\": [get_mean(\"loss_test\", list_evaluate),\n",
    "                                                  get_mean(\"dice_test\", list_evaluate),\n",
    "                                                  get_mean(\"jaccard_test\", list_evaluate),\n",
    "                                                  get_mean(\"precision_test\", list_evaluate),\n",
    "                                                  get_mean(\"recall_test\", list_evaluate)],\n",
    "                                   \"std_test\": [get_std(\"loss_test\", list_evaluate),\n",
    "                                                  get_std(\"dice_test\", list_evaluate),\n",
    "                                                  get_std(\"jaccard_test\", list_evaluate),\n",
    "                                                  get_std(\"precision_test\", list_evaluate),\n",
    "                                                  get_std(\"recall_test\", list_evaluate)],\n",
    "                                   }, index=[\"loss\", \"dice\", \"jaccard\", \"precision\", \"recall\"])\n",
    "dataframe_mean.to_csv(os.path.join(path, \"mean.csv\"), sep=\";\", na_rep=\" \")\n",
    "dataframe_mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for evaluate in list_evaluate:\n",
    "    filename_fold = os.path.join(path, str(evaluate[\"fold\"]), \"metrics.csv\")\n",
    "\n",
    "    dataframe_fold = pandas.DataFrame({\"metrics_train\": [evaluate[\"loss_train\"],\n",
    "                                                  evaluate[\"dice_train\"],\n",
    "                                                  evaluate[\"jaccard_train\"],\n",
    "                                                  evaluate[\"precision_train\"],\n",
    "                                                  evaluate[\"recall_train\"]],\n",
    "                                   \"metrics_val\": [evaluate[\"loss_val\"],\n",
    "                                                  evaluate[\"dice_val\"],\n",
    "                                                  evaluate[\"jaccard_val\"],\n",
    "                                                  evaluate[\"precision_val\"],\n",
    "                                                  evaluate[\"recall_val\"]],\n",
    "                                   \"metrics_test\": [evaluate[\"loss_test\"],\n",
    "                                                  evaluate[\"dice_test\"],\n",
    "                                                  evaluate[\"jaccard_test\"],\n",
    "                                                  evaluate[\"precision_test\"],\n",
    "                                                  evaluate[\"recall_test\"]],\n",
    "                                   }, index=[\"loss\", \"dice\", \"jaccard\", \"precision\", \"recall\"])\n",
    "\n",
    "    dataframe_fold_info = pandas.DataFrame({evaluate[\"fold\"], evaluate[\"time\"]}, index=[\"fold\", \"time\"], columns=[\"info\"])\n",
    "\n",
    "    dataframe_fold = pandas.concat([dataframe_fold, dataframe_fold_info], axis=0)\n",
    "\n",
    "    dataframe_fold.to_csv(filename_fold, sep=\";\", na_rep=\" \")\n",
    "\n",
    "dataframe_fold"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}