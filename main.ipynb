{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import sklearn.model_selection\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, HorizontalFlip, ShiftScaleRotate, ElasticTransform,\n",
    "    RandomBrightness, RandomContrast, RandomGamma\n",
    ")\n",
    "\n",
    "from AugmentationSequence import AugmentationSequence\n",
    "from files import create_folder, save_fit_history, save_lossgraph, save_figs\n",
    "from metrics import dice_coef, jaccard_distance\n",
    "from model import evaluate, unet_model, get_loss_function\n",
    "from save import save"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class CreateSequence(tf.keras.utils.Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        return batch_x, batch_y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GPU"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 3060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-23 17:11:05.539465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            print(f'GPU: {tf.config.experimental.get_device_details(gpu)[\"device_name\"]}')\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'channel': 1,\n",
    "    'batch_size': 8,\n",
    "    'fold': 5,\n",
    "    'epochs': 75,\n",
    "    'image_size': 256,\n",
    "    'learning_rate': 0.001,\n",
    "    'random_state': 1234,\n",
    "    'test_size': 0.2,\n",
    "    'val_size': 0.05,\n",
    "    'path_dataset': '../dataset',\n",
    "    'path_out': 'out',\n",
    "    'loss_function': 'dice',\n",
    "    'data_augmentation': False\n",
    "}\n",
    "images_folder = os.path.join('../dataset_gimp', 'imagens_sp', 'imagens', 'grayscale', 'originais', str(cfg['image_size']), 'jpeg')\n",
    "masks_folder = os.path.join('../dataset_gimp', 'imagens_sp', 'imagens', 'mask', 'mask_manual', str(cfg['image_size']), 'bmp')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "(True, True)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(images_folder), os.path.exists(masks_folder)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375 375 375\n"
     ]
    }
   ],
   "source": [
    "list_labels = list([])\n",
    "list_images = list([])\n",
    "list_images_names = list([])\n",
    "def load_files():\n",
    "    for file in sorted(pathlib.Path(masks_folder).rglob('*')):\n",
    "        mask = tf.keras.preprocessing.image.load_img(file.resolve(), color_mode='grayscale')\n",
    "        mask = tf.keras.preprocessing.image.img_to_array(mask)\n",
    "        mask = mask/255\n",
    "        list_labels.append(mask)\n",
    "\n",
    "        if cfg['channel'] == 1:\n",
    "            image = tf.keras.preprocessing.image.load_img(os.path.join(images_folder, f'{file.stem}.jpeg'), color_mode='grayscale')\n",
    "        else:\n",
    "            image = tf.keras.preprocessing.image.load_img(os.path.join(images_folder, f'{file.stem}.jpeg'))\n",
    "        image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "        image = image/255\n",
    "        list_images.append(image)\n",
    "\n",
    "        list_images_names.append(file)\n",
    "\n",
    "load_files()\n",
    "print(len(list_labels), len(list_images), len(list_images_names))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "(256, 256, 1)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list_images[0].shape\n",
    "list_labels[0].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(375, 256, 256, 1) (375, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.array(list_images).reshape((len(list_images), cfg['image_size'], cfg['image_size'], cfg['channel']))\n",
    "y = np.array(list_labels).reshape((len(list_labels), cfg['image_size'], cfg['image_size'], 1))\n",
    "\n",
    "print(x.shape, y.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(177, 256, 256, 1)\n",
      "(10, 256, 256, 1)\n",
      "(188, 256, 256, 1)\n",
      "(375, 256, 256, 1)\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xandao/miniconda3/lib/python3.8/site-packages/albumentations/augmentations/transforms.py:1613: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n",
      "  warnings.warn(\n",
      "/home/xandao/miniconda3/lib/python3.8/site-packages/albumentations/augmentations/transforms.py:1639: FutureWarning: RandomContrast has been deprecated. Please use RandomBrightnessContrast\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-23 17:11:07.422788: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_170920\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024FlatMapDataset:19779\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - ETA: 0s - loss: 0.5882 - dice_coef: 0.4091 - jaccard_distance: 0.2615 - precision: 0.4150 - recall: 0.8205\n",
      "Epoch 1: val_loss improved from inf to 0.63905, saving model to out/23-10-2022-17-11-06/0/unet.h5\n",
      "12/12 [==============================] - 8s 299ms/step - loss: 0.5882 - dice_coef: 0.4091 - jaccard_distance: 0.2615 - precision: 0.4150 - recall: 0.8205 - val_loss: 0.6391 - val_dice_coef: 0.3609 - val_jaccard_distance: 0.2202 - val_precision: 0.0773 - val_recall: 0.0044 - lr: 0.0010\n",
      "Epoch 2/8\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4326 - dice_coef: 0.5570 - jaccard_distance: 0.3890 - precision: 0.5884 - recall: 0.9784\n",
      "Epoch 2: val_loss improved from 0.63905 to 0.49841, saving model to out/23-10-2022-17-11-06/0/unet.h5\n",
      "12/12 [==============================] - 2s 164ms/step - loss: 0.4326 - dice_coef: 0.5570 - jaccard_distance: 0.3890 - precision: 0.5884 - recall: 0.9784 - val_loss: 0.4984 - val_dice_coef: 0.5016 - val_jaccard_distance: 0.3347 - val_precision: 0.8295 - val_recall: 0.6949 - lr: 0.0010\n",
      "Epoch 3/8\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3838 - dice_coef: 0.6094 - jaccard_distance: 0.4411 - precision: 0.6861 - recall: 0.9777\n",
      "Epoch 3: val_loss did not improve from 0.49841\n",
      "12/12 [==============================] - 2s 144ms/step - loss: 0.3838 - dice_coef: 0.6094 - jaccard_distance: 0.4411 - precision: 0.6861 - recall: 0.9777 - val_loss: 0.6956 - val_dice_coef: 0.3044 - val_jaccard_distance: 0.1795 - val_precision: 0.1499 - val_recall: 0.0154 - lr: 0.0010\n",
      "Epoch 4/8\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3521 - dice_coef: 0.6413 - jaccard_distance: 0.4750 - precision: 0.7578 - recall: 0.9783\n",
      "Epoch 4: val_loss did not improve from 0.49841\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 0.3521 - dice_coef: 0.6413 - jaccard_distance: 0.4750 - precision: 0.7578 - recall: 0.9783 - val_loss: 0.6581 - val_dice_coef: 0.3419 - val_jaccard_distance: 0.2062 - val_precision: 0.5235 - val_recall: 0.0652 - lr: 0.0010\n",
      "Epoch 5/8\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3183 - dice_coef: 0.6741 - jaccard_distance: 0.5111 - precision: 0.7985 - recall: 0.9816\n",
      "Epoch 5: val_loss did not improve from 0.49841\n",
      "12/12 [==============================] - 2s 155ms/step - loss: 0.3183 - dice_coef: 0.6741 - jaccard_distance: 0.5111 - precision: 0.7985 - recall: 0.9816 - val_loss: 0.6106 - val_dice_coef: 0.3894 - val_jaccard_distance: 0.2418 - val_precision: 0.8955 - val_recall: 0.1747 - lr: 0.0010\n",
      "Epoch 6/8\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2945 - dice_coef: 0.6994 - jaccard_distance: 0.5408 - precision: 0.8162 - recall: 0.9842\n",
      "Epoch 6: val_loss did not improve from 0.49841\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 0.2945 - dice_coef: 0.6994 - jaccard_distance: 0.5408 - precision: 0.8162 - recall: 0.9842 - val_loss: 0.5211 - val_dice_coef: 0.4789 - val_jaccard_distance: 0.3149 - val_precision: 0.7694 - val_recall: 0.3687 - lr: 0.0010\n",
      "Epoch 7/8\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2717 - dice_coef: 0.7217 - jaccard_distance: 0.5678 - precision: 0.8364 - recall: 0.9819\n",
      "Epoch 7: val_loss did not improve from 0.49841\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 0.2717 - dice_coef: 0.7217 - jaccard_distance: 0.5678 - precision: 0.8364 - recall: 0.9819 - val_loss: 0.5431 - val_dice_coef: 0.4569 - val_jaccard_distance: 0.2961 - val_precision: 0.8826 - val_recall: 0.3222 - lr: 0.0010\n",
      "Epoch 8/8\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2544 - dice_coef: 0.7397 - jaccard_distance: 0.5904 - precision: 0.8306 - recall: 0.9839\n",
      "Epoch 8: val_loss did not improve from 0.49841\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 0.2544 - dice_coef: 0.7397 - jaccard_distance: 0.5904 - precision: 0.8306 - recall: 0.9839 - val_loss: 0.5040 - val_dice_coef: 0.4960 - val_jaccard_distance: 0.3298 - val_precision: 0.7480 - val_recall: 0.4142 - lr: 0.0010\n",
      "out/23-10-2022-17-11-06/0/fold0-fit.pckl created\n",
      "out/23-10-2022-17-11-06/0/fold0-lossgraph.png created\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 291ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "(178, 256, 256, 1)\n",
      "(10, 256, 256, 1)\n",
      "(187, 256, 256, 1)\n",
      "(375, 256, 256, 1)\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-23 17:11:47.173700: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_210163\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024FlatMapDataset:24706\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - ETA: 0s - loss: 0.6644 - dice_coef: 0.3367 - jaccard_distance: 0.2060 - precision: 0.4551 - recall: 0.4101\n",
      "Epoch 1: val_loss improved from inf to 0.75384, saving model to out/23-10-2022-17-11-06/1/unet.h5\n",
      "12/12 [==============================] - 7s 205ms/step - loss: 0.6644 - dice_coef: 0.3367 - jaccard_distance: 0.2060 - precision: 0.4551 - recall: 0.4101 - val_loss: 0.7538 - val_dice_coef: 0.2462 - val_jaccard_distance: 0.1404 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 2/8\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4840 - dice_coef: 0.5134 - jaccard_distance: 0.3468 - precision: 0.5955 - recall: 0.9307\n",
      "Epoch 2: val_loss did not improve from 0.75384\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 0.4840 - dice_coef: 0.5134 - jaccard_distance: 0.3468 - precision: 0.5955 - recall: 0.9307 - val_loss: 0.9995 - val_dice_coef: 5.3794e-04 - val_jaccard_distance: 2.7285e-04 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 3/8\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4093 - dice_coef: 0.5875 - jaccard_distance: 0.4180 - precision: 0.6336 - recall: 0.9518\n",
      "Epoch 3: val_loss did not improve from 0.75384\n",
      "12/12 [==============================] - 2s 144ms/step - loss: 0.4093 - dice_coef: 0.5875 - jaccard_distance: 0.4180 - precision: 0.6336 - recall: 0.9518 - val_loss: 0.9367 - val_dice_coef: 0.0633 - val_jaccard_distance: 0.0327 - val_precision: 0.1865 - val_recall: 0.0333 - lr: 0.0010\n",
      "Epoch 4/8\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3495 - dice_coef: 0.6492 - jaccard_distance: 0.4821 - precision: 0.7037 - recall: 0.9643\n",
      "Epoch 4: val_loss improved from 0.75384 to 0.66274, saving model to out/23-10-2022-17-11-06/1/unet.h5\n",
      "12/12 [==============================] - 2s 166ms/step - loss: 0.3495 - dice_coef: 0.6492 - jaccard_distance: 0.4821 - precision: 0.7037 - recall: 0.9643 - val_loss: 0.6627 - val_dice_coef: 0.3373 - val_jaccard_distance: 0.2028 - val_precision: 0.3030 - val_recall: 0.1201 - lr: 0.0010\n",
      "Epoch 5/8\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2992 - dice_coef: 0.7008 - jaccard_distance: 0.5412 - precision: 0.8105 - recall: 0.9523\n",
      "Epoch 5: val_loss improved from 0.66274 to 0.60187, saving model to out/23-10-2022-17-11-06/1/unet.h5\n",
      "12/12 [==============================] - 2s 165ms/step - loss: 0.2992 - dice_coef: 0.7008 - jaccard_distance: 0.5412 - precision: 0.8105 - recall: 0.9523 - val_loss: 0.6019 - val_dice_coef: 0.3981 - val_jaccard_distance: 0.2485 - val_precision: 0.3824 - val_recall: 0.8158 - lr: 0.0010\n",
      "Epoch 6/8\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2709 - dice_coef: 0.7301 - jaccard_distance: 0.5763 - precision: 0.8451 - recall: 0.9406\n",
      "Epoch 6: val_loss did not improve from 0.60187\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 0.2709 - dice_coef: 0.7301 - jaccard_distance: 0.5763 - precision: 0.8451 - recall: 0.9406 - val_loss: 0.6792 - val_dice_coef: 0.3208 - val_jaccard_distance: 0.1911 - val_precision: 0.6992 - val_recall: 0.2867 - lr: 0.0010\n",
      "Epoch 7/8\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2359 - dice_coef: 0.7646 - jaccard_distance: 0.6205 - precision: 0.8743 - recall: 0.9526\n",
      "Epoch 7: val_loss improved from 0.60187 to 0.58768, saving model to out/23-10-2022-17-11-06/1/unet.h5\n",
      "12/12 [==============================] - 2s 162ms/step - loss: 0.2359 - dice_coef: 0.7646 - jaccard_distance: 0.6205 - precision: 0.8743 - recall: 0.9526 - val_loss: 0.5877 - val_dice_coef: 0.4123 - val_jaccard_distance: 0.2597 - val_precision: 0.7212 - val_recall: 0.4863 - lr: 0.0010\n",
      "Epoch 8/8\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2041 - dice_coef: 0.7957 - jaccard_distance: 0.6618 - precision: 0.9062 - recall: 0.9561\n",
      "Epoch 8: val_loss improved from 0.58768 to 0.54459, saving model to out/23-10-2022-17-11-06/1/unet.h5\n",
      "12/12 [==============================] - 2s 164ms/step - loss: 0.2041 - dice_coef: 0.7957 - jaccard_distance: 0.6618 - precision: 0.9062 - recall: 0.9561 - val_loss: 0.5446 - val_dice_coef: 0.4554 - val_jaccard_distance: 0.2948 - val_precision: 0.8540 - val_recall: 0.4633 - lr: 0.0010\n",
      "out/23-10-2022-17-11-06/1/fold1-fit.pckl created\n",
      "out/23-10-2022-17-11-06/1/fold1-lossgraph.png created\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    }
   ],
   "source": [
    "kf = sklearn.model_selection.KFold(n_splits=cfg['fold'], shuffle=True, random_state=cfg['random_state'])\n",
    "\n",
    "models = []\n",
    "list_evaluate = list([])\n",
    "list_time = 0\n",
    "current_datetime = datetime.datetime.now().strftime('%d-%m-%Y-%H-%M-%S')\n",
    "path = os.path.join(cfg['path_out'], current_datetime)\n",
    "create_folder(list([path]))\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(x)):\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    x_train, x_val, y_train, y_val = sklearn.model_selection.train_test_split(x_train, y_train, test_size=cfg['val_size'], random_state=cfg['random_state'])\n",
    "\n",
    "    print(x_train.shape)\n",
    "    print(x_val.shape)\n",
    "    print(x_test.shape)\n",
    "    print(x.shape)\n",
    "\n",
    "    path_fold = os.path.join(path, str(fold))\n",
    "    create_folder(list([path_fold]))\n",
    "\n",
    "    augment = Compose([\n",
    "        HorizontalFlip(),\n",
    "        ShiftScaleRotate(rotate_limit=45, border_mode=cv2.BORDER_CONSTANT),\n",
    "        ElasticTransform(border_mode=cv2.BORDER_CONSTANT),\n",
    "        RandomBrightness(),\n",
    "        RandomContrast(),\n",
    "        RandomGamma()\n",
    "    ])\n",
    "    steps_per_epoch = math.ceil(x_train.shape[0] / cfg['batch_size'])\n",
    "    if cfg['data_augmentation']:\n",
    "        train_generator = AugmentationSequence(x_train, y_train, cfg['batch_size'], augment)\n",
    "    else:\n",
    "        train_generator = CreateSequence(x_train, y_train, cfg['batch_size'])\n",
    "    reduce_learning_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, verbose=1)\n",
    "    filename_model = os.path.join(path_fold, 'unet.h5')\n",
    "    checkpointer = tf.keras.callbacks.ModelCheckpoint(filename_model, verbose=1, save_best_only=True)\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "    with strategy.scope():\n",
    "        model = unet_model(cfg)\n",
    "        adam_opt = tf.keras.optimizers.Adam(learning_rate=cfg['learning_rate'])\n",
    "        model.compile(optimizer=adam_opt, loss=get_loss_function(cfg['loss_function']), metrics=[dice_coef, jaccard_distance, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    start_time = time.time()\n",
    "    fit = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=cfg['epochs'], validation_data=(x_val, y_val), callbacks=[checkpointer, reduce_learning_rate])\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    save_fit_history(fold, fit, path_fold)\n",
    "    save_lossgraph(fold, fit, path_fold)\n",
    "    list_evaluate.append(evaluate(end_time, fold, model, x_train, x_val, x_test, y_train, y_val, y_test))\n",
    "    list_time += end_time\n",
    "\n",
    "    models.append(model)\n",
    "\n",
    "    # model = tensorflow.keras.models.load_model('unet_rgb.h5', custom_objects = {'dice_loss': dice_loss, 'dice_coef': dice_coef, 'jaccard_distance': jaccard_distance })\n",
    "\n",
    "    save_figs(cfg, list_images_names, test_index, model, path_fold, x)\n",
    "\n",
    "tf.keras.backend.clear_session()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save(cfg, sys.argv, images_folder, list_evaluate, list_images, list_labels, list_time, masks_folder, path)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
