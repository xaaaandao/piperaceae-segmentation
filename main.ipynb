{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-10 01:20:36.824274: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-10 01:20:36.949412: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-10 01:20:37.439927: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/xandao/miniconda3/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-10-10 01:20:37.439983: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/xandao/miniconda3/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-10-10 01:20:37.439988: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "import sklearn.model_selection\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, HorizontalFlip, ShiftScaleRotate, ElasticTransform,\n",
    "    RandomBrightness, RandomContrast, RandomGamma\n",
    ")\n",
    "\n",
    "from AugmentationSequence import AugmentationSequence\n",
    "from files import create_folder, save_fit_history, save_lossgraph, save_figs\n",
    "from metrics import dice_coef, jaccard_distance\n",
    "from model import evaluate, unet_model, get_loss_function\n",
    "from save import save"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class CreateSequence(tf.keras.utils.Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        return batch_x, batch_y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GPU"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 3060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-10 01:20:38.085154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-10 01:20:38.104958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-10 01:20:38.105117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-10 01:20:38.105844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            print(f'GPU: {tf.config.experimental.get_device_details(gpu)[\"device_name\"]}')\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'channel': 3,\n",
    "    'batch_size': 4,\n",
    "    'fold': 2,\n",
    "    'epochs': 10,\n",
    "    'image_size': 512,\n",
    "    'learning_rate': 0.001,\n",
    "    'random_state': 1234,\n",
    "    'test_size': 0.2,\n",
    "    'val_size': 0.05,\n",
    "    'path_dataset': '../dataset',\n",
    "    'path_out': 'out',\n",
    "    'loss_function': 'dice',\n",
    "    'data_augmentation': False\n",
    "}\n",
    "# images_folder = os.path.join(cfg['path_dataset'], 'imgs', 'CONVERTIDAS', 'RGB', '256', 'OUT')\n",
    "# masks_folder = os.path.join(cfg['path_dataset'], 'new_mask', 'BITMAP', '256', 'OUT')\n",
    "images_folder = \"img\"\n",
    "masks_folder = \"mask\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1\n"
     ]
    }
   ],
   "source": [
    "list_labels = list([])\n",
    "list_images = list([])\n",
    "list_images_names = list([])\n",
    "def load_files():\n",
    "    for file in sorted(pathlib.Path(masks_folder).rglob('*')):\n",
    "        mask = skimage.io.imread(file.resolve())\n",
    "        mask = np.float32(mask/255)\n",
    "        list_labels.append(mask)\n",
    "\n",
    "        image = skimage.io.imread(os.path.join(images_folder, f'{file.stem}.jpeg'))\n",
    "        image = np.float32(image/255)\n",
    "        list_images.append(image)\n",
    "\n",
    "        list_images_names.append(str(file.stem))\n",
    "\n",
    "load_files()\n",
    "print(len(list_labels), len(list_images), len(list_images_names))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(1024, 777)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = skimage.io.imread(\"mask/ALCB002266.bmp\")\n",
    "# list_labels[0].shape\n",
    "a.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 795648 into shape (1,512,512,1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [8], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m x \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(list_images)\u001B[38;5;241m.\u001B[39mreshape((\u001B[38;5;28mlen\u001B[39m(list_images), cfg[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage_size\u001B[39m\u001B[38;5;124m'\u001B[39m], cfg[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage_size\u001B[39m\u001B[38;5;124m'\u001B[39m], cfg[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchannel\u001B[39m\u001B[38;5;124m'\u001B[39m]))\n\u001B[0;32m----> 2\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlist_labels\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mlist_labels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimage_size\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimage_size\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(x\u001B[38;5;241m.\u001B[39mshape, y\u001B[38;5;241m.\u001B[39mshape)\n",
      "\u001B[0;31mValueError\u001B[0m: cannot reshape array of size 795648 into shape (1,512,512,1)"
     ]
    }
   ],
   "source": [
    "x = np.array(list_images).reshape((len(list_images), cfg['image_size'], cfg['image_size'], cfg['channel']))\n",
    "y = np.array(list_labels).reshape((len(list_labels), cfg['image_size'], cfg['image_size'], 1))\n",
    "\n",
    "print(x.shape, y.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kf = sklearn.model_selection.KFold(n_splits=cfg['fold'], shuffle=True, random_state=cfg['random_state'])\n",
    "\n",
    "models = []\n",
    "list_evaluate = list([])\n",
    "list_time = 0\n",
    "current_datetime = datetime.datetime.now().strftime('%d-%m-%Y-%H-%M-%S')\n",
    "path = os.path.join(cfg['path_out'], current_datetime)\n",
    "create_folder(list([path]))\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(x)):\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    x_train, x_val, y_train, y_val = sklearn.model_selection.train_test_split(x_train, y_train, test_size=cfg['val_size'], random_state=cfg['random_state'])\n",
    "\n",
    "    print(x_train.shape)\n",
    "    print(x_val.shape)\n",
    "    print(x_test.shape)\n",
    "    print(x.shape)\n",
    "\n",
    "    path_fold = os.path.join(path, str(fold))\n",
    "    create_folder(list([path_fold]))\n",
    "\n",
    "    augment = Compose([\n",
    "        HorizontalFlip(),\n",
    "        ShiftScaleRotate(rotate_limit=45, border_mode=cv2.BORDER_CONSTANT),\n",
    "        ElasticTransform(border_mode=cv2.BORDER_CONSTANT),\n",
    "        RandomBrightness(),\n",
    "        RandomContrast(),\n",
    "        RandomGamma()\n",
    "    ])\n",
    "    steps_per_epoch = math.ceil(x_train.shape[0] / cfg['batch_size'])\n",
    "    if cfg['data_augmentation']:\n",
    "        train_generator = AugmentationSequence(x_train, y_train, cfg['batch_size'], augment)\n",
    "    else:\n",
    "        train_generator = CreateSequence(x_train, y_train, cfg['batch_size'])\n",
    "    reduce_learning_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, verbose=1)\n",
    "    filename_model = os.path.join(path_fold, 'unet.h5')\n",
    "    checkpointer = tf.keras.callbacks.ModelCheckpoint(filename_model, verbose=1, save_best_only=True)\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "    with strategy.scope():\n",
    "        model = unet_model(cfg)\n",
    "        adam_opt = tf.keras.optimizers.Adam(learning_rate=cfg['learning_rate'])\n",
    "        model.compile(optimizer=adam_opt, loss=get_loss_function(cfg['loss_function']), metrics=[dice_coef, jaccard_distance, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    start_time = time.time()\n",
    "    fit = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=cfg['epochs'], validation_data=(x_val, y_val), callbacks=[checkpointer, reduce_learning_rate])\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    save_fit_history(fold, fit, path_fold)\n",
    "    save_lossgraph(fold, fit, path_fold)\n",
    "    list_evaluate.append(evaluate(end_time, fold, model, x_train, x_val, x_test, y_train, y_val, y_test))\n",
    "    list_time += end_time\n",
    "\n",
    "    models.append(model)\n",
    "\n",
    "    # model = tensorflow.keras.models.load_model('unet_rgb.h5', custom_objects = {'dice_loss': dice_loss, 'dice_coef': dice_coef, 'jaccard_distance': jaccard_distance })\n",
    "\n",
    "    save_figs(cfg, list_images_names, test_index, model, path_fold, x)\n",
    "\n",
    "tf.keras.backend.clear_session()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save(cfg, sys.argv, images_folder, list_evaluate, list_images, list_labels, list_time, masks_folder, path)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
